


# Distributive numbers:  a neo-Baroque perspective on probability

Adrian Mackenzie,

Sociology, Lancaster University

Bailrigg, LA14YL, UK

a.mackenzie@lancaster.ac.uk

ph (44) 01524 594184


\newpage

## Introduction

> "We ran the election 66,000 times every night," said a senior official, describing the computer simulations the campaign ran to figure out Obama's odds of winning each swing state. "And every morning we got the spit-out — here are your chances of winning these states. And that is how we allocated resources." [@scherer_how_2012]

In the US Presidential elections of November 2012, the data analysis team supporting the re-election of Barack Obama were said to be running a statistical model of the election 66,000 times every night [@scherer_how_2012]. Their model, relying on polling data, records of past voting behaviour, and many other databases, was guiding tactical decisions about everything from where the presidential candidate would speak, where advertising money would be spent, to the telephone calls that targeted individual citizens (for donations or their vote).  Widely reported in television news and internationally in print media (_Time_, _New York Times_, _The Observer_), the outstanding feature of Obama's re-election seems to me to be the figure of 66,000 nightly model runs. Why so many thousand runs? This question was not addressed in the media reports, nor surprisingly, addressed in the online discussion on blogs and other online forums that followed. A glimmering of an answer appears in more extended accounts of the Obama data analytics efforts [@issenberg_definitive_2012] that describe how, in contrast to the much smaller and traditional market research-based targeting of demographic groups used by the Republican campaign for Mitt Romney, the Obama re-election campaign focused on knowing, analysing and predicting what *individuals* would do in the election. In post-demographic understandings of data, individuals rather than populations or sub-populations, appear as such. How can individuals appear in models? The answer is to be found, I suggest, in a decisive shift in probability practices that distributes numbers differently in the world. Hardly ever discussed in accounts of the growth of big data,  shifts in the role played by probability change the meaning and value of data as such, and hence, everything that depends on data. 

## $Pr(A)$: events and beliefs in the world

A standard textbook of statistics introduces the idea of probability as event-related number in this way:

> We will assign a real number $Pr(A)$ to every event $A$, called the **probability** of A [@Wasserman_2003, 3]

Note that this number is 'real', so it can take infinitely many values between 0 and 1. The number concerns 'events', where events are understood as subsets of all the possible outcomes in a given 'sample space' ('the **sample space** $\Omega$ is the set of possible outcomes of an experiment.  ... Subsets of $\Omega$ are called **Events**' [@Wasserman_2003,3]).   Wasserman goes on to say: 

> There are many interpretations of $Pr(A)$. The common interpretations are frequencies and degrees of belief. ... The difference in interpretation will not matter much until we deal with statistical inference. There the differing interpretations lead to two schools of inference: the frequentists and Bayesian schools [@Wasserman_2003, 6]. 

The difference will only matter, suggests Wasserman, in relation to the style of statistical inference. However, it may be that even apart from the different interpretations of probability,  the practice of assigning numbers to events in $\Omega$ harbours some surprising variations. 

Summarising his own account of the emergence of probability, the philosopher and historian Ian Hacking writes:

>I claimed in _The Emergence of Probability_ that our idea of probability is a  Janus-faced mid-seventheenth-century mutation in the Renaissance idea of signs. It came into being with a frequency aspect and a degree-of-belief aspect [@Hacking_1990, 96].

In the work from 1975, Hacking, writing largely prior to the shifts in probability practice I discuss, claims that there was no probability prior to 1660 [@Hacking_1975]. Not only is probability a Baroque invention, the fundamental instability that permits recent mutations in probability practice  has a distinctively Baroque flavour in the way that it combines something happening in the world with something that pertains to subjects. As we can read in statistics textbooks, there is nothing controversial in Hacking's claim that probability is Janus-faced. Historian of statistics and statisticians themselves regularly describe about probability as bifurcated in the same way. Statisticians commonly contrast the frequentist and degree-of-belief, the *aleatory* and the *epistemic*, views of probability. Although the history of statistics shows various distributions and permutations of emphasis on the subjective and objective versions of probability, statisticians are now relatively happily normalised around a divided view of probability.

Contemporary probability, however, has become entwined with a particular mode of computation that deeply convolutes the difference between the epistemic and aleatory faces of probability. The techniques involved here include the bootstrap [@Efron_1975], expectation-maximisation [@Dempster_1977], and Markov-Chain Monte Carlo [@Gelfand_1990].  These techniques support increasingly post-demographic treatments of populations, in which for instance, individuals increasingly attract probability distributions, as in Obama's data-intensive re-election campaign.[^1] In examining a salient contemporary treatment of probability, my concern is the problem of invention of forms of thought able to critically affirm mutations in probability today. These mutations arise, I suggest, in many, perhaps all, contemporary settings where populations, events, numbers and calculation are to be found. In seeking to unfold ways of thinking probability for social theory from computational practice, risks of scientism or scientocentrism abound. On this score, a Baroque sense of the enfolding of inside and outside, of belief and events, of approximation and exactitudes offers at least tentative pointers to a different way of describing what is happening as aleatory and the epistemic senses of probability find themselves recombined. 

[^1]: This chapter will not trace the complicated historical emergence of probability and its development in various statistical approaches to knowing, deciding, classifying, normalising, governing, breeding, predicting and modelling. Historians of statistics have documented this in great detail, and tracked how statistics is implicated in power-knowledge in various settings [@Mackenzie_1978;@Stigler_1986; @Hacking_1990; @Daston_1994; @Porter_1996]. 

## Exact means simulated

```{r gibbs_normal_bivar, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, fig.cap = 'Gibbs sampling of bivariate normal distribution', dpi=400} 

	source('mcmc_examples.R')
	gibbs_normal_bivariate()
```

The contour plot in Figure \ref{gibbs_normal_bivar}  was generated by the widely used statistical simulation technique called MCMC -- Markov Chain Monte Carlo simulation. MCMC has greatly transformed much statistical practices since the early 1990s. The diagram shows the contours of two normally-distributed sets of numbers as they vary in relation to each other. The topography of this diagram is the product of a simulation of specific kinds of numbers, in this case, the mean values of two normal distributions. The contour lines trace  the different values of the means ($\mu_1, \mu_2$) of the variables. For the time being, we need know nothing about what such peaks refer to, apart from the fact they are something to do with probability, with assigning numbers to events. A set of connected points starting on the side of the one of the peaks and clustering on the peak mark the traces of the itinerary of the MCMC algorithm as it explores the topography in search of peaks that represent more likely events or beliefs. 


Invented during the 1950s, the MCMC technique is important in contemporary statistics, and especially in Bayesian statistics [@Gelman_2003]. It plays significant roles in applications such as image, speech and audio processing, computer vision, computer graphics, molecular biology and genomics, robotics, decision theory and information retrieval [@Andrieu_2003, 37-38], but above all, in general statistical modelling, inference and prediction. Usually called an *algorithm* -- a series of precise operations that transform or  reshape data --  MCMC has been called one of 'the ten most influential algorithms' in twentieth century science and engineering [@Andrieu_2003, 5].[^2] But MCMC is not really an algorithm, or at least, if it is, it is an algorithm subject to substantially different algorithmic implementations (for instance, Metropolis-Hastings and Gibbs Sampler are two popular implementations). In all of these settings, MCMC is a way of simulating a sample of points distributed on a complicated curve or surface (see Figure \ref{gibbs_normal_bivar}). The MCMC technique addresses the problem of how to explore and map very uneven or folded distributions of numbers. It is a way of navigating areas or volumes whose curves, convolutions and hidden recesses elude geometrical spaces and perspectival vision. Accounts of MCMC emphasise the 'high-dimensional' spaces in which the algorithm works: ‘there are several high-dimensional problems, such as computing the volume of a convex body in *d* dimensions, for which MCMC simulation is the only known general approach for providing a solution within a reasonable time’ [@Andrieu_2003,5]. We might say that MCMC alongside other statistical algorithms such as the bootstrap or EM increasingly facilitates the envisioning of high-dimensional, convoluted data spaces. Simulating the distribution of numbers over folded surfaces, MCMC  renders the areas and volumes of folds more amenable to calculation. 

[^2]: In making sense of the change described by Robert and Casella, scientific histories of the technique are useful.  The brief version of the history of MCMC might run as follows:  physicists working on nuclear weapons at Los Alamas in the 1940s [@Metropolis_1949]} first devised ways of working with high-dimensional spaces in statistical mechanical approaches to physical processes such as crystallisation and nuclear fission and fusion. Their approach to statistical mechanics was later generalised by statisticians [@Hastings_1970]}. It was   taken up by ecologists working on spatial interactions in plant communities during the 1970s [@Besag_1974],  revamped by computer scientists working on blurred image reconstruction [@geman_stochastic_1984], and then subsequently seized on again by statisticians in the early 1990s [@Gelfand_1990]. In the 1990s, it became clear that the algorithm could make Bayesian inference — a general style of statistical reasoning that differs substantially from mainstream statistics in its treatment of probability [@Mcgrayne_2011] — practically useable in many situations. A vast, still continuing, expansion of Bayesian statistics ensued, nearly all of which relied on MCMC in some form or other. (Thompson Reuters Web of Knowledge shows 6 publications on MCMC in 1990, but over 1000 *each year* for the last five years in areas ranging from agricultural economics to zoology, from wind-power capacity prediction to modelling the decline of lesser sand eels in the North Sea; similarly NCBI Pubmed lists close to 4000 MCMC-related publications since 1990 in biomedical and life sciences, ranging from classification of new-born babies EEGs to within-farm transmission of foot and mouth disease; searches on 'Bayesian' yield many more results).  

What MCMC adds to the world is subtle yet indicative. In their history of the technique, Christian Robert and George Casella, two leading statisticians specializing in  MCMC,  write that  ‘Markov chain Monte Carlo changed our emphasis from “closed form” solutions to algorithms, expanded our impact to solving “real” applied problems and to improving numerical algorithms using statistical ideas, and led us into a world where “exact” now means “simulated”’ [@Robert_2008,18].  This shift from ‘closed form’ solution to algorithms and to a world where ‘exact means simulated’ might be all too easily framed by a post-modern sensibility as another example of the primacy of the simulacra over the original. But here, a Baroque sensibility, awake to the convolution of objective-event and subjective-event senses of probability, might allow us to approach MCMC less in terms of a crisis of referentiality, and more in terms of the emergence of a new form of distributive number. 

How so? The contours of Figure \ref{gibbs_bivar_norm} define a volume. In its typical usages, the somewhat complicated shape of this  volume typically equates to a probability. MCMC, put in terms of the minimal formal textbook definition of probability is a way of assigning real numbers to events, but according to a mapping shaped by the convoluted folds of such volumes. The identification of $Pr(A)$ with a convoluted volume offers great potential to statistics. For instance,  political scientists regularly use MCMC in their work because their research terrain — elections, opinions, voting patterns — little resembles the image of events projected by mainstream statistics: independent, identically distributed (’iid’) events staged in experiments.  MCMC allows, as the political scientist Jeff Gill observes, all unknown quantities to be ‘treated probabilistically’ [@Gill_2011,1]. We can begin to glimpse  why the Obama re-election team might have been running their model 66,000 times each night. In short, MCMC allows, at least in principle,  *every* number to be treated as a probability distribution.

##$\frac{1}{\infty}$: distributed individuals as random variables

Let us return to the typical problem of the individual voter as envisaged by the Obama re-election team. Treating every number as a probability distribution involves an exteriorisation of numbers in the service of an individualising interiorisation of probability.  Techniques of statistical simulation multiply numbers in the world and  assign numbers to events, but largely in the service of modifying, limiting, quantifying uncertainties associated with belief. This folding together of subjective and objective, of epistemic and aleatory senses of probability can be thought as a neo-Baroque mode of probability. The Baroque sense of probability, especially as articulated by G.W. Leibniz, the 'first philosopher of probability' [@Hacking_1975, 57], is helpful in holding together these contrapuntal movements. Leibniz’s famously impossible claim that each monad includes the whole world is, according to Gilles Deleuze, actually a claim about numbers in variation. Through numbers, understood in a somewhat unorthodox way, monads — the parts of the world — [HERE]  can include the whole world. This is an especially slippery point in Deleuze’s analysis of Leibniz’s work, but one that has strong contemporary resonances. Deleuze says: ‘for Leibniz, the monad is clearly the most “simple” number, that is, the inverse, reciprocal, harmonic number’ [@deleuze_fold_1993, 129]. These kinds of philosophical formulations raise many questions , but the very possibility of individualising a number into the root, or the radix of the monad, seems to me quite important if we are interested in developing ways of engaging affirmatively with number, without granting it the pure ontological primacy of mathesis or exiling it to the badlands of alienated reason.  

Having a world — for the monad is a mode of having a world by including it — as a number entails a very different notion of *having* and a somewhat different notion of number. The symbolic expression of this inclusion is:

>$\frac{1}{\infty}$

The numerator points to the single individual, the denominator, $\infty$, suggests a world. The fraction or ratio of 1 to $\infty$ tends towards a vanishingly small difference (zero), yet one whose division passes through all numbers (the whole world). In this process of convergence towards zero (rather than infinity), Deleuze writes that for in the Baroque, ‘the painting-window [of Renaissance perspective] is replaced by tabulation, the grid on which lines, numbers and changing characters are inscribed. … Leibniz’s monad would be just a such grid’ (27). This suggests a different notion of the subject, no longer the subject of the world-view who sees along straight lines that converge at an infinite distance (the subject as locus of reason, experience or intentionality), but as ‘the truth of a variation’ (20) played out in numbers and characters tabulated on gridded screens. Alongside the individual voters modelled by the Obama re-election team, we might think of border control officers viewing numerical, predictions of whether a particular passenger arriving on a flight is likely to present a security risk [@amoore_lines_2009], financial traders viewing changing prices for a currency or financial derivative on their screens [@knorr-cetina_traders_2002], a genomic researcher deciding whether the alignment scores between two different DNA sequences suggests a phylogenetic relationship, or a player in a large online multiplayer games such as World of Warcraft quickly checking the fatigue levels of their character before deciding what to do: these are all typical cases where numbers in variation populate the monadic grid. But there is also a shift in the understanding of number here. In relation to the monad as inverse number, Deleuze writes ‘the inverse number has special traits: it is infinite or infinitely small, but also, by opposition to the natural number, which is collective, it is individual and distributive’ (129). Here too, the suggestion that numbers possess traits such as being ‘individual and distributive’ rather than collective resonates with contemporary transformations in probability practice. Like the connected points trekking toward the peak in a MCMC computation, distributive numbers -- such as contemporary probabilities -- move along  supple lines  through experience, and across coarse distinctions between subject-object, nature-culture, self-other. 


