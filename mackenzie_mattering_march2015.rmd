


# Distributive numbers:  a neo-Baroque perspective on probability

Adrian Mackenzie,

Sociology, Lancaster University

Bailrigg, LA14YL, UK

a.mackenzie@lancaster.ac.uk

ph (44) 01524 594184


\newpage

## Introduction

> "We ran the election 66,000 times every night," said a senior official, describing the computer simulations the campaign ran to figure out Obama's odds of winning each swing state. "And every morning we got the spit-out — here are your chances of winning these states. And that is how we allocated resources." [@scherer_how_2012]

In the US Presidential elections of November 2012, the data analysis team supporting the re-election of Barack Obama were said to be running a statistical model of the election 66,000 times every night [@scherer_how_2012]. Their model, relying on polling data, records of past voting behaviour, and many other databases, was guiding tactical decisions about everything from where the presidential candidate would speak, where advertising money would be spent, to the telephone calls that targeted individual citizens (for donations or their vote).  Widely reported in television news and internationally in print media (_Time_, _New York Times_, _The Observer_), the outstanding feature of Obama's re-election seems to me to be the figure of 66,000 nightly model runs. Why so many thousand runs? This question was not addressed in the media reports, nor surprisingly, addressed in the online discussion on blogs and other online forums that followed. A glimmering of an answer appears in more extended accounts of the Obama data analytics efforts [@issenberg_definitive_2012] that describe how, in contrast to the much smaller and traditional market research-based targeting of demographic groups used by the Republican campaign for Mitt Romney, the Obama re-election campaign focused on knowing, analysing and predicting what *individuals* would do in the election. In post-demographic understandings of data, individuals rather than populations or sub-populations, appear as such. How can individuals appear in models? The answer is to be found, I suggest, in a decisive shift in probability practices that distributes numbers differently in the world. Hardly ever discussed in accounts of the growth of big data,  shifts in the role played by probability change the meaning and value of data as such, and hence, everything that depends on data. 

## $Pr(A)$: events and beliefs in the world

A standard textbook of statistics introduces the idea of probability as event-related number in this way:

> We will assign a real number $Pr(A)$ to every event $A$, called the **probability** of A [@Wasserman_2003, 3]

Note that this number is 'real', so it can take infinitely many values between 0 and 1. The number concerns 'events', where events are understood as subsets of all the possible outcomes in a given 'sample space' ('the **sample space** $\Omega$ is the set of possible outcomes of an experiment.  ... Subsets of $\Omega$ are called **Events**' [@Wasserman_2003,3]).   Wasserman goes on to say: 

> There are many interpretations of $Pr(A)$. The common interpretations are frequencies and degrees of belief. ... The difference in interpretation will not matter much until we deal with statistical inference. There the differing interpretations lead to two schools of inference: the frequentists and Bayesian schools [@Wasserman_2003, 6]. 

The difference will only matter, suggests Wasserman, in relation to the style of statistical inference. However, it may be that even apart from the different interpretations of probability,  the practice of assigning numbers to events in $\Omega$ harbours some surprising variations. 

Summarising his own account of the emergence of probability, the philosopher and historian Ian Hacking writes:

>I claimed in _The Emergence of Probability_ that our idea of probability is a  Janus-faced mid-seventheenth-century mutation in the Renaissance idea of signs. It came into being with a frequency aspect and a degree-of-belief aspect [@Hacking_1990, 96].

In the work from 1975, Hacking, writing largely prior to the shifts in probability practice I discuss, claims that there was no probability prior to 1660 [@Hacking_1975]. Not only is probability a Baroque invention, the fundamental instability that permits recent mutations in probability practice  has a distinctively Baroque flavour in the way that it combines something happening in the world with something that pertains to subjects. As we can read in statistics textbooks, there is nothing controversial in Hacking's claim that probability is Janus-faced. Historian of statistics and statisticians themselves regularly describe about probability as bifurcated in the same way. Statisticians commonly contrast the frequentist and degree-of-belief, the *aleatory* and the *epistemic*, views of probability. Although the history of statistics shows various distributions and permutations of emphasis on the subjective and objective versions of probability, statisticians are now relatively happily normalised around a divided view of probability.

Contemporary probability, however, has become entwined with a particular mode of computation that deeply convolutes the difference between the epistemic and aleatory faces of probability. The techniques involved here include the bootstrap [@Efron_1975], expectation-maximisation [@Dempster_1977], and Markov-Chain Monte Carlo [@Gelfand_1990].  These techniques support increasingly post-demographic treatments of populations, in which for instance, individuals increasingly attract probability distributions, as in Obama's data-intensive re-election campaign.[^1] In examining a salient contemporary treatment of probability, my concern is the problem of invention of forms of thought able to critically affirm mutations in probability today. These mutations arise, I suggest, in many, perhaps all, contemporary settings where populations, events, numbers and calculation are to be found. In seeking to unfold ways of thinking probability for social theory from computational practice, risks of scientism or scientocentrism abound. On this score, a Baroque sense of the enfolding of inside and outside, of belief and events, of approximation and exactitudes offers at least tentative pointers to a different way of describing what is happening as aleatory and the epistemic senses of probability find themselves recombined. 

[^1]: This chapter will not trace the complicated historical emergence of probability and its development in various statistical approaches to knowing, deciding, classifying, normalising, governing, breeding, predicting and modelling. Historians of statistics have documented this in great detail, and tracked how statistics is implicated in power-knowledge in various settings [@Mackenzie_1978;@Stigler_1986; @Hacking_1990; @Daston_1994; @Porter_1996]. 

## Exact means simulated

```{r gibbs_normal_bivar, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, fig.cap = 'Gibbs sampling of bivariate normal distribution', dpi=400} 

	source('mcmc_examples.R')
	gibbs_normal_bivariate()
```

The contour plot in Figure \ref{gibbs_normal_bivar}  was generated by the widely used statistical simulation technique called MCMC -- Markov Chain Monte Carlo simulation. MCMC has greatly transformed much statistical practices since the early 1990s. The diagram shows the contours of two normally-distributed sets of numbers as they vary in relation to each other. The topography of this diagram is the product of a simulation of specific kinds of numbers, in this case, the mean values of two normal distributions. The contour lines trace  the different values of the means ($\mu_1, \mu_2$) of the variables. For the time being, we need know nothing about what such peaks refer to, apart from the fact they are something to do with probability, with assigning numbers to events. A set of connected points starting on the side of the one of the peaks and clustering on the peak mark the traces of the itinerary of the MCMC algorithm as it explores the topography in search of peaks that represent more likely events or beliefs. 

When ‘Sampling-Based Approaches to Calculating Marginal Densities,’ the article that first announced the arrival of MCMC in statistical practice [@Robert_2010, 9] appeared in _Journal of the American Statistical Association_ in 1990 [@Gelfand_1990], the  statisticians Alan Gelfand and Adrian Smith stated that the problem they were addressing was  how ‘to obtain numerical estimates of non-analytically available marginal densities of some or all [the collection of random variables] simply by means of simulated samples from available conditional distributions, and without recourse to sophisticated numerical analytic methods’ (398). Their formulation emphasises the mixture of using  things that are accessible  -- simulated samples -- to explore things that are not directly accessible -- 'non-analytically available marginal densities' (some of this probability terminology will be explored below). For present purposes, the important point is a newly non-analytical probability is in formation here. It lies at some distance from the classical probability calculus first developed in the 17th century around games of chance, mortality statistics and the like.

Note that these statisticians are not announcing the invention of a new technique. They explicitly take up the already existing Gibbs sampler algorithm for image-processing, as described in [@Geman_1984],  investigate some of its formal properties (convergence), and then set out a number of mainstream statistical problems that could be done differently using MCMC and the Gibbs sampler in particular. They show how MCMC facilitates Bayesian statistical inference  by re-configuring six illustrative mainstream statistical examples: multinomial models, hierarchical models, multivariate normal sampling, variance components, and the k-group normal means model. The illustrations in the paper suggest how previously difficult problems of statistical inference can be carried out by sampling simulations. As they state in another paper from the same year, ‘the potential of the methodology is enormous, rendering straightforward the analysis of a number of problems hitherto regarded as intractable’ [@Gelfand_1990, 984].[^3]

[^3]:  A rapid convergence on MCMC follows from the 1990s onwards. Gibbs samplers appear in desktop computer software such as the widely used WinBUGS ('Windows Bayes Using Gibbs Sampler') written by statisticians at Cambridge University in the early 1990s [@Lunn_2000], and MCMC quickly moves into the different disciplines and applications found today.

Note too that while  the MCMC technique has become important in contemporary statistics, and especially in Bayesian statistics [@Gelman_2003], it plays significant roles in applications such as image, speech and audio processing, computer vision, computer graphics, molecular biology and genomics, robotics, decision theory and information retrieval [@Andrieu_2003, 37-38] HERE.  Usually called an *algorithm* -- a series of precise operations that transform or  reshape data --  MCMC has been called one of 'the ten most influential algorithms' in twentieth century science and engineering [@Andrieu_2003, 5].[^2] But MCMC is not really an algorithm, or at least, if it is, it is an algorithm subject to substantially different algorithmic implementations (for instance, Metropolis-Hastings and Gibbs Sampler are two popular implementations). In all of these settings, MCMC is a way of simulating a sample of points distributed on a complicated curve or surface (see Figure \ref{gibbs_normal_bivar}). The MCMC technique addresses the problem of how to explore and map very uneven or folded distributions of numbers. It is a way of navigating areas or volumes whose curves, convolutions and hidden recesses elude geometrical spaces and perspectival vision. Accounts of MCMC emphasise the 'high-dimensional' spaces in which the algorithm works: ‘there are several high-dimensional problems, such as computing the volume of a convex body in *d* dimensions, for which MCMC simulation is the only known general approach for providing a solution within a reasonable time’ [@Andrieu_2003,5]. We might say that MCMC alongside other statistical algorithms such as the bootstrap or EM increasingly facilitates the envisioning of high-dimensional, convoluted data spaces. Simulating the distribution of numbers over folded surfaces, MCMC  renders the areas and volumes of folds more amenable to calculation. 

[^2]: In making sense of the change described by Robert and Casella, scientific histories of the technique are useful.  The brief version of the history of MCMC might run as follows:  physicists working on nuclear weapons at Los Alamas in the 1940s [@Metropolis_1949]} first devised ways of working with high-dimensional spaces in statistical mechanical approaches to physical processes such as crystallisation and nuclear fission and fusion. Their approach to statistical mechanics was later generalised by statisticians [@Hastings_1970]}. It was   taken up by ecologists working on spatial interactions in plant communities during the 1970s [@Besag_1974],  revamped by computer scientists working on blurred image reconstruction [@geman_stochastic_1984], and then subsequently seized on again by statisticians in the early 1990s [@Gelfand_1990]. In the 1990s, it became clear that the algorithm could make Bayesian inference — a general style of statistical reasoning that differs substantially from mainstream statistics in its treatment of probability [@Mcgrayne_2011] — practically useable in many situations. A vast, still continuing, expansion of Bayesian statistics ensued, nearly all of which relied on MCMC in some form or other. (Thompson Reuters Web of Knowledge shows 6 publications on MCMC in 1990, but over 1000 *each year* for the last five years in areas ranging from agricultural economics to zoology, from wind-power capacity prediction to modelling the decline of lesser sand eels in the North Sea; similarly NCBI Pubmed lists close to 4000 MCMC-related publications since 1990 in biomedical and life sciences, ranging from classification of new-born babies EEGs to within-farm transmission of foot and mouth disease; searches on 'Bayesian' yield many more results).  

What MCMC adds to the world is subtle yet indicative. In their history of the technique, Christian Robert and George Casella, two leading statisticians specializing in  MCMC,  write that  ‘Markov chain Monte Carlo changed our emphasis from “closed form” solutions to algorithms, expanded our impact to solving “real” applied problems and to improving numerical algorithms using statistical ideas, and led us into a world where “exact” now means “simulated”’ [@Robert_2008,18].  This shift from ‘closed form’ solution to algorithms and to a world where ‘exact means simulated’ might be all too easily framed by a post-modern sensibility as another example of the primacy of the simulacra over the original. But here, a Baroque sensibility, awake to the convolution of objective-event and subjective-event senses of probability, might allow us to approach MCMC less in terms of a crisis of referentiality, and more in terms of the emergence of a new form of distributive number. 

How so? The contours of Figure \ref{gibbs_bivar_norm} define a volume. In its typical usages, the somewhat complicated shape of this  volume typically equates to a probability. MCMC, put in terms of the minimal formal textbook definition of probability is a way of assigning real numbers to events, but according to a mapping shaped by the convoluted folds of such volumes. The identification of $Pr(A)$ with a convoluted volume offers great potential to statistics. For instance,  political scientists regularly use MCMC in their work because their research terrain — elections, opinions, voting patterns — little resembles the image of events projected by mainstream statistics: independent, identically distributed (’iid’) events staged in experiments.  MCMC allows, as the political scientist Jeff Gill observes, all unknown quantities to be ‘treated probabilistically’ [@Gill_2011,1]. We can begin to glimpse  why the Obama re-election team might have been running their model 66,000 times each night. In short, MCMC allows, at least in principle,  *every* number to be treated as a probability distribution.

##$\frac{1}{\infty}$: distributed individuals as random variables

Let us return to the typical problem of the individual voters modelled by the Obama re-election team. Treating every number as a probability distribution involves an exteriorisation of numbers in the service of an individualising interiorisation of probability.  Techniques of statistical simulation multiply numbers in the world and  assign numbers to events, but largely in the service of modifying, limiting, quantifying uncertainties associated with belief. This folding together of subjective and objective, of epistemic and aleatory senses of probability can be thought as a neo-Baroque mode of probability. The Baroque sense of probability, especially as articulated by G.W. Leibniz, the 'first philosopher of probability' [@Hacking_1975, 57], is helpful in holding together these contrapuntal movements. Leibniz’s famously impossible claim that each monad _includes_ the whole world is, according to Gilles Deleuze, actually a claim about numbers in variation. Through numbers, understood in a somewhat unorthodox way, monads — the parts of the world —  can include the whole world. Deleuze says: ‘for Leibniz, the monad is clearly the most “simple” number, that is, the inverse, reciprocal, harmonic number’ [@Deleuze_1993, 129]. 

Having a world — for the monad is a mode of having a world by including it — as a number entails a very different notion of *having* and a somewhat different notion of number. The symbolic expression of this inclusion is, according to Deleuze:

>$\frac{1}{\infty}$

The numerator $1$ points to the singular individual (remember taht for Leibniz, every monad is individual), the denominator, $\infty$, suggests a world. The fraction or ratio of 1 to $\infty$ tends towards a vanishingly small difference (zero), yet one whose division passes through all numbers (the whole world). In what sense is this fraction, in its convergence towards zero, including a world? Deleuze writes that for in the Baroque, ‘the painting-window [of Renaissance perspective] is replaced by tabulation, the grid on which lines, numbers and changing characters are inscribed. … Leibniz’s monad would be just a such grid’ (27). This suggests a different notion of the subject, no longer the subject of the world-view who sees along straight lines that converge at an infinite distance (the subject as locus of reason, experience or intentionality), but as ‘the truth of a variation’ (20) played out in numbers and characters tabulated on gridded screens. The monad is a grid of numbers and characters in variation. How could we concretise this?  Alongside the individual voters modelled by the Obama re-election team, we might think of border control officers viewing numerical, predictions of whether a particular passenger arriving on a flight is likely to present a security risk [@Amoore_2009], financial traders viewing changing prices for a currency or financial derivative on their screens [@Knorr-cetina_2002], a genomic researcher deciding whether the alignment scores between two different DNA sequences suggests a phylogenetic relationship, or a player in a large online multiplayer games such as World of Warcraft quickly checking the fatigue levels of their character before deciding what to do: these are all typical cases where numbers in long chains of converging variation populate the monadic grid. $\frac{1}{\infty}$ entails a significant shift in the understanding of number.  Deleuze writes that ‘the inverse number has special traits: ... by opposition to the natural number, which is collective, it is individual and distributive’ (129). If numbers become ‘individual and distributive,’ then the calculations that produce them might be important to map in the specificity of their transformations.

```{r  distributions, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Distributions'} 

	source('mcmc_examples.R')
	generate_distributions()
```

Earlier we saw the flat operational definition of probability as assigning real numbers between 0 and 1 to events. By contrast,  a random variable 'is a mapping that assigns a real number to each outcome’ [@Wasserman_2003,19], but this number can vary. If events have probabilities, random variables comprehend a range of outcomes that are mapped to numbers in the form of probability distributions. The practical reality of random variables is variation, variations that are usually take the visual forms of the curves of the  probability distributions shown in Figure \ref{fig:distributions}. These distributions each have their own history (see [@Stigler_1986] for a detail historical account of key developments), but for our purposes the important points are both historical and philosophical. On the one hand, the historical development of probability distributions, particularly the Gaussian or normal distribution, but also lesser known Poisson, Beta or hypergeometric distributions, displays powerful inversions in which the mapping of numbers to events becomes a mapping of events to numbers. Hacking, for instance, describes how the 19th century statistician Adolphe Quetelet began to treat populations. The normal distribution 'became a reality underneath the phenomena of consciousness' [@Hacking_1990, 205].  A whole set of normalizations, often with strongly biopolitical dynamics hinges on this inversion of the relation between numbers and events in 19th century probability practice.

\begin {equation}
\label {eq:gaussian}
f(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end {equation}

On the other hand, the regularity, symmmetry and above all mathematical expression of these functions in equations such as the one shown in Equation \ref{eq:gaussian} more or less delimited statistical practice. Such expressions offer great tractability since their shape, area or volume can all be expressed in terms of key tendencies such as $\mu$, the mean and $\signm$, the variance. The 18th and 19th century development of statistical practice pivots on manipulations that combine or generalize such expressions to an increasing variety of situations. For instance, in Figure \ref{fig:gibbs_bivariate_normal}, the normal distribution shown in Equation \ref{eq:gaussian} for one variable $x$ becomes a bi-variate normal distribution for two variables $x_1$ and $x_2$. Nevertheless, these equations also limit the range of shapes, areas and volumes that statistical practice could map onto events. When statisticians speak of 'fitting a density' (a probability distribution) to data, they affirm their commitment to the regular forms of probability distributions.

## An endless flow of random variables

Both aspects of this commitment -- the curve as underlying reality of events, and the normalized expression of curves in functions whose parameters shape the curve -- begin to shift in techniques such as MCMC. In particular, following Deleuze's discussion of the monad as distributive number, we might say that the probability distributions now function less as the collective form of individuals, and more as the distributive form of individuals. What could this mean in practice?   

MCMC inaugurates 'a world where “exact” now means “simulated”' [@Robert_2008,18]. This comment links an epistemic quality -- exactitude -- with a calculative, modelling process -- simulation. But more specifically, MCMC is, as mentioned above, a technique for simulating samples from complicated concave surfaces and volumes that arise when random variables map complicated outcomes. [HERE]  In other words, it is a way of exploring the contoured and folded surfaces generated when flows of data or random variables come together in one joint probability distribution. These surfaces, generated by the combinations of mathematical functions or probability distributions are not easy to see or explore,  except in the exceptional cases where calculus can deliver a deductive analytical ‘closed form’ solution to the problems of integration (finding the area) and differentiation (finding the distribution function for one variable). By contrast, MCMC effectively simulates some important parts of the surface, and in simulating convoluted volumes, loosens the analytical ties that bind probability to certain well-characterised analytical regular forms such as the normal curve. 

In this simulation of folded and multiplied probability distributions, the lines between objective and subjective, or aleatory and epistemic probability, begin to shift. There is perhaps something increasingly monadological about MCMC, as we can see if we revisit the history of the technique with less an eye on the events leading up to the [Bayesian] revolution, and more with an eye on what is being folded in, and what  is unfolding as the technique develops. The starting point here, and it is found in almost every textbook on MCMC-related methods is the computer as random number generator. Rather than Peirce's 'chance pouring in at every sense,' it might be better to speak of chance pouring out of MCMC on every event. 


 




