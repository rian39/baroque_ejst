

# Distributive numbers:  a neo-Baroque perspective on probability

Adrian Mackenzie,

Sociology, Lancaster University

Bailrigg, LA14YL, UK

a.mackenzie@lancaster.ac.uk

ph (44) 01524 594184


## Abstract

>Since its Baroque invention [@hacking_emergence_1975], probability has been a double-sided coin. On one side, it concerns degrees of belief (the so-called 'subjective' view), and on the other side, frequencies, or how often things happen in the world (the so-called 'objective' view). In the last few centuries, one side of this coin has come up more often -- the frequency version of probability. Yet, as many historians of statistics, and statisticians themselves recognise, probability as degree of belief has never disappeared. It has only occurred less often, and been less often the object of belief. This ineluctable entwining of belief and events, of subjective-objective faces, in probability seems quintessentially Baroque in its interweaving and folding together of inside and outside. Drawing on contemporary statistical practice, and Gilles Deleuze's understanding of monads as 'simple, inverse, distributive numbers' [@deleuze_fold_1993], this paper examines the resurgence of the probability as degree of belief in the face of a world seemingly teeming with data. It argues that in the last few decades of statistical practice associated especially with 'Bayesian inference' and the techniques of Markov Chain Monte Carlo (MCMC) simulation, we see a re-configured and super-imposed concept of probability taking shape. As these practices pervade diverse scientific fields, commerce, government and industry, we might be seeing a different epistemic materialisation taking shape in which beliefs and events are less separate. On the contrary, through computation, subjective belief is exteriorised in simulated events, and a certain staging of events are reshaped as updateable beliefs. 


\newpage

## Introduction

> "We ran the election 66,000 times every night," said a senior official, describing the computer simulations the campaign ran to figure out Obama's odds of winning each swing state. "And every morning we got the spit-out — here are your chances of winning these states. And that is how we allocated resources." [@scherer_how_2012]

In the US Presidential elections of November 2012, the data analysis team supporting the re-election of Barack Obama were said to be running a statistical model of the election 66,000 times every night [@scherer_how_2012]. Their model, relying on polling data, records of past voting behaviour, and many other databases, was guiding tactical decisions about everything from where the presidential candidate would speak, where advertising money would be spent, to the telephone calls that targeted individual citizens (for donations or their vote).  Widely reported in television news and internationally in print media (_Time_, _New York Times_, _The Observer_), the outstanding feature of Obama's re-election seems to me to be the figure of 66,000 nightly model runs. Why so many thousand runs? This question was not addressed in the media reports, nor surprisingly, addressed in the online discussion on blogs and other online forums that followed. A glimmering of an answer appears in more extended accounts of the Obama data analytics efforts [@issenberg_definitive_2012] that describe how, in contrast to the much smaller and traditional market research-based targeting of demographic groups used by the Republican campaign for Mitt Romney, the Obama re-election campaign focused on knowing, analysing and predicting what *individuals* would do in the election. This is one amongst many recent illustrations of the post-demographic power attributed to data. In post-demographic understandings of data, individuals rather than populations or sub-populations, appear as such. How can individuals appear in models? The answer is to be found, I suggest, in a decisive shift in probability practices. Hardly ever discussed in media accounts of the growth of big data,  certain shifts in the role played by probability change the meaning and value of data as such, and hence, everything that depends on data. 

A Baroque perspective helps in describing these recent mutations in probability.  Summarising his own account of the emergence of probability, the philosopher and historian Ian Hacking writes:

>I claimed in _The Emergence of Probability_ that our idea of probability is a  Janus-faced mid-seventheenth-century mutation in the Renaissance idea of signs. It came into being with a frequency aspect and a degree-of-belief aspect [@hacking_taming_1990, 96].

Indeed, in the work from 1975, Hacking, writing largely prior to the shifts in probability practice I discuss, claims that there was no probability prior to 1660 [@hacking_emergence_1975]. Not only is probability a Baroque invention, the fundamental instability that permits recent mutations in probability practice  has a distinctively Baroque flavour in the way that it combines something happening in the world with something that pertains to subjects. There is nothing controversial in Hacking's claim that probability is Janus-faced. Historian of statistics and statisticians themselves regularly speak about probability in the same way. They commonly contrast the frequentist and degree-of-belief, the *aleatory* and the *epistemic*, views of probability. Although the history of statistics shows various distributions and permutations of emphasis on the subjective and objective versions of probability, statisticians are now relatively happily normalised around a divided view of probability. 

For instance, a well-regarded textbook of statistics written by Larry Wasserman introduces the idea of probability as event-related number in this way:

> We will assign a real number $Pr(A)$ to every event $A$, called the **probability** of A [@wasserman_all_2003,3]

Note that this number is 'real', meaning that it can take infinitely many values between 0 and 1; secondly, the number concerns events, where events are understood as subsets of all the possible outcomes in a given 'sample space' ('the **sample space** $\Omega$ is the set of possible outcomes of an experiment.  ... Subsets of $\Omega$ are called **Events**' [@wasserman_all_2003,3]).   Wasserman goes on to say: 

> There are many interpretations of $Pr(A)$. The common interpretations are frequencies and degrees of belief. ... The difference in interpretation will not matter much until we deal with statistical inference. There the differing interpretations lead to two schools of inference: the frequentists and Bayesian schools [@wasserman_all_2003, 6]. 

The difference will only matter, suggests Wasserman, in relation to the style of statistical inference. However, it may be that even before the different interpretations of probability come into play,  the practice of assigning numbers to events in $\Omega$ secretly transforms numbers. 

## Markov Chain Monte Carlo: an algorithm for subjectifying probability objectively?

Contemporary probability has become entwined with a particular mode of computation that convolutes the difference between the epistmic and aleatory faces of probability. This convolution supports increasingly post-demographic treatments of population, in which for instance, individuals attract probability distributions, as in Obama's data-intensive re-election campaign. This paper will not in any way trace the complicated historical emergence of probability and its development in various statistical approaches to knowing, deciding, classifying, normalising, governing, breeding, predicting and modelling. Historians of statistics have documented this in great detail, and tracked how statistics is implicated in power-knowledge in various settings [@mackenzie_statistical_1978;@stigler_history_1986; @hacking_taming_1990; @daston_how_1994; @porter_trust_1996]. In examining a salient contemporary treatment of probability, my concern is the problem of invention of forms of thought able to critically affirm mutations in probability today. These mutations arise, I suggest, in many, perhaps all, contemporary settings where populations, events, numbers and calculation are to be found. In seeking to unfold ways of thinking probability for social theory from computational practice, risks of scientism or scientocentrism abound. On this score, a Baroque sense of what happens offers at least tentative pointers to a different way of describing what is happening as aleatory and the epistemic senses of probability find themselves recombined. 

```{r gibbs_normal_bivar, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, fig.cap = 'Gibbs sampling of bivariate normal distribution', dpi=400} 

	source('mcmc_examples.R')
	gibbs_normal_bivariate()
```

The contour plot in Figure 1 was generated by a  statistical simulation technique called MCMC -- Markov Chain Monte Carlo simulation -- that has greatly transformed much statistical practices since the early 1990s (see [@mcgrayne_theory_2011] for a popular account). This very simple simulation of the contours of two normally-distributed sets of numbers shows two main things. The contour lines trace  the different values of the means ($\mu_1, \mu_2$) of the variables. For the time being, we need know nothing about what such peaks refer to, apart from the fact they are something to do with probability, with assigning numbers to events. A set of connected points starting on the side of the one of the peaks and clustering on the peak shows how the MCMC algorithm explores the contours. Peaks -- zones that attract events or beliefs -- are sometimes difficult to find in complicated terrain. MCMC is a way of finding peaks.  

Invented during the 1950s, the MCMC technique is important in contemporary statistics, and especially in Bayesian statistics. It plays significant roles in applications such as image, speech and audio processing, computer vision, computer graphics, molecular biology and genomics, robotics, decision theory and information retrieval [@andrieu_introduction_2003, 37-38], but above all, in general statistic modelling. MCMC is usually called an *algorithm*: a series of precise operations that transform or  reshape data. Moreover, MCMC has been called one of 'the ten most influential algorithms' in twentieth century science and engineering [@andrieu_introduction_2003, 5]. But MCMC is not really an algorithm, or at least, if it is, it is an algorithm subject to various algorithmic implementations (for instance, Metropolis-Hastings and Gibbs Sampler are two popular implementations).

In all of these settings, MCMC is a way of simulating a sample of points distributed on a complicated curve or surface (see Figure 1). The MCMC technique addresses the problem of how to sense or feel  very uneven or folded distributions of numbers. It is a way of calculating areas or volumes whose curves, convolutions and hidden recesses elude geometrical spaces and perspectival vision. Accounts of MCMC emphasise the 'high-dimensional' spaces in which the algorithm works: ‘there are several high-dimensional problems, such as computing the volume of a convex body in *d* dimensions, for which MCMC simulation is the only known general approach for providing a solution within a reasonable time’ [@andrieu_introduction_2003,5]. Indeed, we could say that MCMC increasingly facilitates the fabrication of high-dimensional, convoluted data spaces. Simulating a sample of points on folded surfaces, it becomes possible to calculate the area or volume enclosed by the surface. This area or volume typically equates to a probability. MCMC, put in terms of the minimal formal definition of probability is a way of assigning real numbers to events, but events occurring within complicated sample spaces. 

What MCMC has added to the world is subtle yet indicative. In a history of the technique, Christian Robert and George Casella, two leading statisticians specializing in  MCMC,  write that  ‘Markov chain Monte Carlo changed our emphasis from “closed form” solutions to algorithms, expanded our impact to solving “real” applied problems and to improving numerical algorithms using statistical ideas, and led us into a world where “exact” now means “simulated”’ [@robert_history_2008,18].  This shift from ‘closed form’ solution to algorithms and a world where ‘exact means simulated’ might be all too easily framed by a post-modern sensibility as another example of the primacy of the simulacra over the original. But here, a Baroque sensibility, awake to the at once objective and subjective senses of probability, might allow us to approach MCMC less precipitously and less in terms of a crisis of referentiality. 

In making sense of the change described by Robert and Casella, scientific histories of the technique are useful.  The brief version of the history of MCMC might run as follows:  physicists working on nuclear weapons at Los Alamas in the 1940s [@metropolis_monte_1949]} first devised ways of working with high-dimensional spaces in statistical mechanical approaches to physical processes such as crystallisation and nuclear fission and fusion. Their approach to statistical mechanics was later generalised by statisticians [@hastings_monte_1970]}. It was   taken up by ecologists working on spatial interactions in plant communities during the 1970s [@besag_spatial_1974],  revamped by computer scientists working on blurred image reconstruction [@geman_stochastic_1984], and then subsequently seized on again by statisticians in the early 1990s [@gelfand_sampling-based_1990]. In the 1990s, it became clear that the algorithm could make Bayesian inference — a general style of statistical reasoning that differs substantially from mainstream statistics in its treatment of probability [@mcgrayne_theory_2011] — practically useable in many situations. A vast, still continuing, expansion of Bayesian statistics ensued, nearly all of which relied on MCMC in some form or other. (Thompson Reuters Web of Knowledge shows 6 publications on MCMC in 1990, but over 1000 *each year* for the last five years in areas ranging from agricultural economics to zoology, from wind-power capacity prediction to modelling the decline of lesser sand eels in the North Sea; similarly NCBI Pubmed lists close to 4000 MCMC-related publications since 1990 in biomedical and life sciences, ranging from classification of new-born babies EEGs to within-farm transmission of foot and mouth disease; searches on 'Bayesian' yield many more results).  In the social sciences too, political scientists regularly use MCMC in their work because their research terrain — elections, opinions, voting patterns — little resembles the image of events projected by mainstream statistics: independent, identically distributed (’iid’) events staged in experiments. When brought together with Bayesian inference, MCMC allows, as the political scientist Jeff Gill observes, all unknown quantities to be ‘treated probabilistically’ [@gill_introduction_2011,1]. We can begin to see why the Obama re-election team might have been running their model 66,000 times each night. In short, MCMC allows, at least in principle,  *every* number to be treated as a probability. This a key shift in the probability practice, and one that opens the way to post-demographic conceptions of individuation. 

A more general affirmation of probabilities is not unprecedented, at least philosophically. As Hacking reports, C.S Peirce, the American pragmatist philosopher who spent much of his life measuring things for the US Coastal Survey, was already arguing against *any* constant numbers, social or natural, in the late nineteenth century [@hacking_taming_1990, 200]. Only statistical stabilities mattered. A century later, the popularisation of MCMC perhaps surpasses what Peirce (and Hacking?) had in mind in saying there are only statistical stabilities. Peirce envisioned a universe filled with chance events ('chance pours in at every sense'), amidst which islands of pragmatic sense emerged standing on habit and consensus. By contrast, treating every number as a probability, as facilitated by MCMC, does not simply generalise probability by saying that the world is indeed aleatory, and that our beliefs are rolling stones on the bed of a fast-flowing stream. On the contrary, as I will seek to show, it allows a hyper-subjectified sense of probability to take shape precisely through its exteriorisation in folded flows of random numbers. A technique of computational simulation distributes numbers in the world, it assigns numbers of events, but largely in the service of modifying, limiting, quantifying uncertainties associated with belief. This folding together of subjective and objective, of epistemic and aleatory senses of probability can be thought as a neo-Baroque monadological mode of probability.

## Distributions, individuals and random variables

Again, the Baroque sense of probability, especially as articulated by G.W. Leibniz, the 'first philosopher of probability' [@hacking_emergence_1975, 57], is helpful in keeping matters more open. Let us return to the typical problem of the individual voter as envisaged by the Obama re-election team. Leibniz’s famously impossible claim that each monad includes the whole world is, according to Gilles Deleuze, actually a claim about numbers in variation. Through numbers, understood in a somewhat unorthodox way, monads — the parts of the world —  can include the whole world. This is an especially slippery point in Deleuze’s analysis of Leibniz’s work, but one that has strong contemporary resonances. Deleuze says: ‘for Leibniz, the monad is clearly the most “simple” number, that is, the inverse, reciprocal, harmonic number’ [@deleuze_fold_1993, 129]. These kinds of philosophical formulations raise many questions , but the very possibility of individualising a number into the root, or the radix of the monad, seems to me quite important if we are interested in developing ways of engaging affirmatively with number, without granting it the pure ontological primacy of mathesis or exiling it to the badlands of alienated reason.  

Having a world — for the monad is a mode of having a world by including it — as a number entails a very different notion of *having* and a somewhat different notion of number. The symbolic expression of this inclusion is:

>$\frac{1}{\infty}$

The numerator points to the single individual, the denominator, $\infty$, suggests a world. The fraction or ratio of 1 to $\infty$ tends towards a vanishingly small difference (zero), yet one whose division passes through all numbers (the whole world). In this process of convergence towards zero (rather than infinity), Deleuze writes that for in the Baroque, ‘the painting-window [of Renaissance perspective] is replaced by tabulation, the grid on which lines, numbers and changing characters are inscribed. … Leibniz’s monad would be just a such grid’ (27). This suggests a different notion of the subject, no longer the subject of the world-view who sees along straight lines that converge at an infinite distance (the subject as locus of reason, experience or intentionality), but as ‘the truth of a variation’ (20) played out in numbers and characters tabulated on gridded screens. Alongside the individual voters modelled by the Obama re-election team, we might think of border control officers viewing numerical, predictions of whether a particular passenger arriving on a flight is likely to present a security risk [@amoore_lines_2009], financial traders viewing changing prices for a currency or financial derivative on their screens [@knorr-cetina_traders_2002], a genomic researcher deciding whether the alignment scores between two different DNA sequences suggests a phylogenetic relationship, or a player in a large online multiplayer games such as World of Warcraft quickly checking the fatigue levels of their character before deciding what to do: these are all typical cases where numbers in variation populate the monadic grid. But there is also a shift in the understanding of number here. In relation to the monad as inverse number, Deleuze writes ‘the inverse number has special traits: it is infinite or infinitely small, but also, by opposition to the natural number, which is collective, it is individual and distributive’ (129). Here too, the suggestion that numbers possess traits such as being ‘individual and distributive’ rather than collective resonates with contemporary transformations in probability practice. Like the connected points trekking toward the peak in a MCMC computation, distributive numbers -- such as contemporary probabilities -- move along  supple lines  through experience, and across coarse distinctions between subject-object, nature-culture, self-other. 

From a Baroque perspective, the flat operational definition of probability as mapping events to real numbers seethes with convolutions.  Everywhere, numbers are increasingly in variation, and display increasingly distributional characteristics.  While some events have discrete outcomes (Obama was re-elected in 2012), many happenings are not. In Lancaster here in north-west England, the probability of rain on a given day would be say 70%, but days have very different amounts of rain.  Some days, it rains once briefly and lightly. Other days it rains frequently and heavily. A gamut of rain events can occur, and each would distribute different amounts of water on Lancaster. Given the amount of variation,  a much better way to face the weather is to say that Lancaster's rain is a *random variable*: ‘a random variable is a mapping that assigns a real number to each outcome’ [@wasserman_all_2003,19]. If events have probabilities, random variables have a range of outcomes that are mapped to numbers. Again, the deceptive simplicity of 'mapping' hides many variations. Mapping is a form of one-to-one correspondence, usually expressed as a mathematical function. A random variable links events to numbers through functions.  Again, all this remains rather formal. The practical reality of random variables is variation, variations that are  visually expressed by curves and by probability distributions. 

```{r  distributions, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Distributions'} 

	source('mcmc_examples.R')
	generate_distributions()
```
Probability distributions are a common way of showing and  talking about random variables. The curves shown in Figure 2 could refer to almost anything (the chances of rain at different times of day in Lancaster, the seasonal variation in precipitation, etc.). These distributions appear in countless shapes and forms in scientific, government and popular literature of many different kinds. Statistical graphics have a rich history and semiology that I do not discuss here (see [@bertin_semiology_1983]). Perhaps the most famous function or mapping is the normal or Gaussian distribution:

>$f(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$

This  function, whose mathematics were intensively worked over during the 18-19th centuries, has a power-laden biopolitical history closely tied with knowledges and governing of  national and other  populations. The key symbols here include $\mu$, the mean and $\sigma$, the variance. These two parameters together describe  the shape of the curve. Given $\mu$ and $\sigma$, it become possible to map different outcomes to probabilities. Given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. For instance, if an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak in Figure 2 (and hence less than the population mean), they quickly render them somehow subject to the normal curve. But statistics uses dozens of different probability distributions to map continuous and discrete variations to real numbers. Other probability distributions abound — normal (Gaussian), uniform, Cauchy exponential, gamma, beta,  hypergeometric, binomial, Poisson, chi-squared, Boltzmann-Gibbs distributions, etc (see [@nist_2012] for a gallery of distributions) — because outcomes occur in widely differing patterns. The  queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Queues are usually modelled using a Poisson distribution, which unfortunately for travellers, distributes waiting times very differently.  Similarly, it might be better to think of the probability of rain today in Lancaster in terms of a Poisson distribution that models that queue of clouds in the Atlantic just waiting to land on the northwest coast of England. Rather than addressing the question of if it will rain or not, a Poisson-based model might address the question of how soon.

The diverse range of probability distributions — and we will see below some reasons why we can expect them to proliferate in certain settings — attests to the variety of ways in which events might be mapped to real numbers. Despite  the sometime forbidding mathematical equations, the term *distribution* emphasises a quite material or tangible way of thinking about how probabilities vary. The curves in both Figure 1 and Figure 2 are examples of the most common mathematical descriptions in any data analysis setting: they are  *probability density* functions (pdf). (There are also  *probability mass* functions for variables that have discrete values; for instance: 1,2,3,4,5). Pdfs such as the Gaussian function shown above are usually graphed as a curve that indicates how likely a random variable is to take on a particular value. In many cases, statistical practice seeks to estimate distribution functions such as pdfs (or their close relatives, cdfs — *cumulative distribution functions*) for the given data. Statisticians speak of 'fitting a density' to data, emphasising their assumption that events can be incorporated in the forms of probability distributions. The underlying probability distribution is in principle ‘unobservable’ as such, but a probability density function is  assumed to give rise to all the variations in data gathered through experiments and observations. The task is to estimate the shape of that curve, and its defining parameters (means, variance, etc.). Given that curve, areas under the pdf equate to the likely range of value of a variable. While the total 'probability mass' under the probability density function curve always must be equal to one (since the combined probability of all possible outcomes = 1), finding the area under particular parts of the curve is a key issue. Finding the area under probability density curves becomes the way in which many epistemic processes envisage lived states of affairs as random variables or as numbers in variation.

## Multiplying curves

In the Bayesian statistics popular since the 1990s, any number, including the defining parameters of other distributions such as the mean, can be treated as a random variable. In principle, any number becomes probabilistic, that is, expressible as a probability distribution. Indeed, the 'Bayesian revolution' in statistics pivots on multiplying probabilities in both senses of the term: proliferation and combining in a  product. Bayes Theorem, known since the 18th century, is usually presented in the first few pages of any probability textbook, as a way of relating probabilities to each other by multiplying them together:

>$Pr(A|B) = \frac{Pr(B|A) Pr(A)}{Pr(B)}$

where  $Pr(A)$ and $Pr(B)$ are two probabilities, and $Pr(A\vert{B})$ is the probability of $A$ given $B$, and $Pr(B\vert{A})$,  conversely, is the probability of $B$ given $A$. Again, bearing in mind the different notions of probability discussed above (subjective - objective; degree of belief vs. frequency of outcome), this formula can be read in different ways. Bayes Theorem expresses the relation between probabilities through multiplication. If the $A$ and $B$ are random variables, then the curves and lines of their respective probability densities are being multiplied to produce higher-dimensional surfaces, as we saw in the very simple illustration of Figure 1 where $A$ and $B$ were random variables described by a Gaussian function.  The *joint probability distribution* that results from the multiplication of individual probability densities still has the same total mass (1), but now distributed in a different volume. As the number of random variables grows, the surfaces open onto higher dimensions, and cannot be graphed easily. 

Knowledge of how to mathematically manipulate the functions  associated with particular probability distributions has accreted over several centuries. They all share a common purpose: to express the distribution of outcomes associated  with certain events. Bringing together random variables in models,  even in the basic form of the Bayes Rule where $B$ is conditioned by $A$, means  multiplying probability density functions. The total mass of the the probability always remains the same (i.e. 1), but the question is where it is distributed. As  the simulated joint probability density of Figure 1 shows, certain zones of a joint probability are much more elevated than others, and these peaks suggest more likely events in the range of possible outcomes.

The mathematical difficulties posed as different random variables or probability cross-hatch each other relates to that other great Baroque mathematical invention, calculus. If calculus made possible so many different calculations of rates of change, calculations that profoundly affected senses of space, time, and increasingly growth, variation and change more generally (hence, Deleuze’s work both on Leibniz and in his philosophical conceptualisation of difference more generally is deeply imbricated with differential calculus [@deleuze_fold_1993]), it also ran into many obstacles in relations to calculations of probability. Calculating the area under a curve in order to estimate variables is a problem of integration. That is, the area under a curve is given  by the *integral* of the probability density function. If the probability distribution cannot be normalized, then the area under the curve is much harder to estimate. The ornate and at times bewildering apparatus of statistical tests and procedures found in statistics (t-test, the Wald test, Pearson's $\chi^2$ test for multinomial data, analysis of variance, etc), as well as the antagonisms between different schools of statistics (Bayesian vs frequentist), largely obscures the continuous trajectory that transforms the problem  of probability  into a problem of measuring areas under curves, or volumes under surfaces. Sometimes estimates of probability are understood as a measure of our belief about what happens (as in Bayesian analysis) and sometimes it is understood as a measure of the frequency with which events occur in the world (as in frequentist statistics). Although there is now a very extensive technical and philosophical literature on the differences between Bayesian and frequentist statistics, more or less the same computations can be in the service of either standpoint. So this is not the main point I want to pursue here. 

From the perspective of a Baroque sensibility, mathematical functions for working with different shapes, areas, densities and masses of probability distributions have been combined to support estimations, inferences and predictions of change and growth in many processes. Through the generating role played by probability distributions in almost any field of science, government, industry, technology and increasingly media and commerce we could name, probability mixes through almost all forms of relationality. In calculations of insurance risk, in algorithms for error correction, in psychological testing, in climate models or biodiversity surveys, just to name a few, probability distributions ground all inference. Although certain distributions, such as the normal, Poisson or binomial, etc., have dominated in these developments, this was largely because it has been easier to calculate estimates of their main parameters (mean, variance, etc) than those pertaining to less familiar distributions. In terms of shape, area and hence probability density, the normal distribution is one of the most tractable curves to work with. Even with the various data transformations and normalizations developed over several centuries, other probability distributions have been harder to work with. (They lack the 'closed form' solutions that Robert and Casella refer to.) This occasions many disputes in the history of statistics over ‘curve-fitting’ to normal or other mathematically tractable distributions as arbitrary and unjustified [@hacking_taming_1990, 164]. In whatever way these disputes have been resolved (see [@mackenzie_statistical_1978] for an early 20th century example), the practical problem of calculating the area under all or some part of the curve has skewed what we believe about many different things (about sub-atomic particles, climate change, likelihood of glaucoma, the chances of rain today, Obama's chance of re-election, etc.) towards some forms of probabilitity distribution more than others. The normal probability density function tends to be the norm.

## Good approximations to probabilities

The proliferation of normal curves and surfaces brings us back to MCMC, the technique that inaugurates 'a world where “exact” now means “simulated”' [@robert_history_2008,18]. MCMC is, as mentioned above, a technique for simulating samples from high-dimensional or complicated concave volumes. In other words, it is a way of exploring the contoured and folded surfaces generated when flows of data or random variables come together in one joint probability distribution. These surfaces, generated by the combinations of mathematical functions or probability distributions are not easy to see or explore,  except in the exceptional cases where calculus can deliver a deductive analytical ‘closed form’ solution to the problems of integration (finding the area) and differentiation (finding the distribution function for one variable). By contrast, MCMC effectively simulates some important parts of the surface, and in simulating convoluted volumes, loosens the analytical ties that bind probability to certain well-characterised analytical regular forms such as the normal curve. 

In this simulation of folded and multiplied probability distributions, the lines between objective and subjective, or aleatory and epistemic probability, begin to shift. There is perhaps something increasingly monadological about MCMC, as we can see if we revisit the history of the technique with less an eye on the events leading up to the [Bayesian] revolution, and more with an eye on what is being folded in, and what  is unfolding as the technique develops. The starting point here, and it is found in almost every textbook on MCMC-related methods is the computer as random number generator. Rather than Peirce's 'chance pouring in at every sense,' it might be better to speak of chance pouring out of MCMC on every event. 

```{r generate_distributions, echo=FALSE, warning=FALSE, fig.cap='Simulated distributions'} 
	source('mcmc_examples.R')
	generate_beta_distribution()
```
Figure 3 shows two plots. The one on the left plots 10,000 computer generated random numbers between 0 and 1, and as expected, or hoped, they are more less uniformly distributed between 0 and 1. This is simulation of the simplest probability distribution of all, the *uniform* probability distribution in which all events are equally likely. The plot on the right derives from the same random numbers, but shows a different probability distribution in which events mapped to numbers close to 0 are much more likely than events close to 1. What has happened here? The reshaping of the flow of numbers depends on a very simple multiplication of the simulated uniform distribution by itself:  

> A real function of a random variable is another random variable. Random variables with a wide variety of distributions can be obtained by transforming a standard uniform random variable $U \approx UNIF(0, 1)$ 
. Let $U \approx UNIF(0, 1)$.
> ... We seek the distribution of $X = U^2$ [@suess_introduction_2010, 32].

It happens that multiplying a uniform distribution by itself ($U^2$) produces an instance of another important distribution, the *Beta* distribution, shown on the right of Figure 3. Now it would be possible to produce that curve of a beta distribution analytically, by plotting points generated by the *Beta* probability density function:

>$f(x; \alpha, \beta)= constant \bullet x^{\alpha-1}(1-x)^\beta-1$

where $\alpha=0.5$ and $\beta=1$. But in the case of the plots shown on the right of Figure 3, the shape has been generated from a flow of random variables, So, from a flow of random numbers, generated by the computer (using an *pseudo-random* number generator algorithm), more random variables result, but with different shapes or probability densities.  As Robert and Casella write, 'the point is that a supply of random variables can be used to generate different distributions' [@robert_introducing_2010,p.44]. Indeed, this is the principle of all Monte Carlo simulations, methods that 'rely on the possibility of producing (with a computer) a supposedly endless flow of random variables for well-known or new distributions' [@robert_introducing_2010, 42]. The example  shown here is really elementary in terms of the distribution and dimensionality of the random variables involve, but it illustrates a general practice underpinning the MCMC technique: the reshaping of the 'supposedly endless flow of random variables' to produce known or new distributions.  

This practice is already monadological in the sense that it seems to bring probability inside the computer. Monte Carlo simulations render computers as substitutes for events in the world, and they render that world more manipulable by knowing subjects. It is hardly surprising that scientists working at the epicentre of the ‘closed world’ [@edwards_closed_1996] of post-WWII nuclear weapons research should develop a technique that allows the world to move in this way. In 1953, Metropolis, the Rosenbluths and the Tellers were calculating ‘the properties of any substance which may be considered as composing of interacting individual molecules’ [@metropolis_equation_1953, 1087]  (for instance, the flux of neutrons in a hydrogen bomb detonation). In their short, but still widely cited paper (over 20,000 citations according to Google Scholar; over 14,000 according to Thomson Reuters Web of Knowledge), they describe how they used computer simulation to deal with the number of possible interactions in a substance, and to thereby come up with a statistical description of the properties of the substance. Their model system consists of a square containing only a few hundred particles. These particles are at various distances from each other and exert forces (electric, magnetic, etc.) on each other dependent on the distance. In order to estimate the probability that the substance will be in any particular state (fissioning, vibrating, crystallising, cooling down,  etc.), they needed to integrate over the many dimensional space comprising all the distance and forces between the particles. (This space is a typical multivariate joint distribution.) As they write, ‘it is evidently impossible to carry out a several hundred dimensional integral by the usual numerical methods, so we resort to the Monte Carlo method’ (1088), a method that Nicholas Metropolis and Stanislaw Ulam had already described in an earlier paper [@metropolis_monte_1949]. Here the problem is that the turbulent randomness of events in a square containing a few hundred particles thwarts calculations of the physical properties of the substance. They substitute for that non-integrable turbulent randomness a controlled flow of random variables generated by a computer. While still somewhat random (i.e. pseudo-random), these Monte Carlo variables taken together approximate to the integral of the many dimensional space. In monadological terms, Monte Carlo simulation attenuates the distance between the aleatory and epistemic poles of probability. While the computer is regarded as aleatory in its capacity to generate seemingly random numbers, it is strongly epistemic in its power to marshall these numbers into shapes that cannot be analysed using the 'usual numerical methods' (for instance, of integral calculus). 

Metropolis and co-authors immediately go on to say, however, that they cannot just sample a random set of points. The range of events is not equally accessible to simulation. The joint probability distribution of the system is quite uneven (that is, highly folded). Randomly sampled points are likely to lie in low probability regions (valleys and plains), whereas they are interested in the high probability peaks.  Instead they propose a move which becomes the modus operandi of subsequent MCMC work (and hence justifies the high citation count): ‘we place the N particles in any configuration … then we move each of particles in succession’ (1088). Here is the beginning of the ‘random walk’ or 'Markov chain' technique that distinguishes MCMC from Monte Carlo simulation more generally. As well as generating a sample of random variables, they submit each variable to a test. Physically, the image here is that they displace each particle by a small random amount. Having moved the particle/variable, they  calculate the resulting slight change in the overall system state, and then decide whether that particular move puts the system in a more or less probable state. If that state is more likely, the move is allowed; otherwise the particle goes back to  where it was. Having carried out this process of small moves for all the particles, they can calculate the overall system state or property.  The process of randomly displacing the particles by a small amount, and always moving to the more probable states, effectively explores the bumpy topography of the joint probability density. In many minute moves, the simulation begins to migrate all the randomly generated values points towards the peaks that represent interestingly high probabilities.

```{r metrohast_normal, echo=FALSE, fig.cap='Bivariate normal distribution generated using MCMC'} 
	generate_normal_bivariate_metro_hastings()
```

In Figure 4, a toy example, the MCMC technique has been used to generate a simulate a bivariate normal probability distribution. (The example comes from [@suess_introduction_2010, 177-178].) We have already seen a bivariate normal distribution (see Figure 1), but now the distribution has been produced by drawing on  the uniformly distributed flow of random variables produced as an MCMC algorithm (actually in this case, the Metropolis-Hastings implementation of MCMC) runs, rather than by generating values using the mathematical function for the probability density. On the left hand side, we see the path by taken a single random variable as it moves closer to the central peak of the distribution, the zone of highest probability.  On the right hand side, we can see the cloud of variables clustered around the central peak of the bivariate normal distribution after the MCMC technique has run 40,000 times. The Metropolis-Hastings implementation and the perhaps more popular Gibbs sampler implementation of the MCMC technique share this idea of waiting to see where flows of random variables end up and hoping that distribution of these values will approximate a 'a desired long-run distribution' [@suess_introduction_2010, 150]. 

## How are events and beliefs combined in MCMC? Probability in  Markov Chains

'Consider the Markov chain defined by $X^{t+1} = \sigma X^{t} + \epsilon(t)$ where $\epsilon(t) ~ \mathcal{U}=(0,1)$', write Robert & Casella [@robert_introducing_2010, p.169]. This Markov chain knows nothing of the normal distribution, yet simulates it by piling up large numbers of random numbers. 

```{r markov_chain, echo=FALSE, cache=TRUE, fig.cap='Markov-chain generated normal distribution', dpi=400}
	source('mcmc_examples.R')
	generate_markov()
```

It is likely that this is something that  the Obama data analytics team were doing each night. For present purposes, however, the point is that these supplementary moves - the slight perturbations and adjustments that permit a kind of exploration of the topography of the probability density surface -- remain monadological in the sense that they express a world within a computational setting. This is not to say that the world is clearly and distinctly expressed in a closed monad. The simulated world is not immediately accessible or transparent. It has to be explored through the invention of techniques that render its variations and differences somehow sensible. 

Morever, even though MCMC suggests that events might be simulated in order to estimate their probabilities, it practically acts as a way of modifying existing beliefs through data. We already glimpsed this in Bayes Theorem: it multiplies random variables  With apologies for the slightly forbidding typography, the probability:

>$f(\mathbf{x}\vert\theta) = L(\theta) = \prod_{i}f(x_i\vert\theta)$

is a likelihood function, a function that describes how likely some data (from measurements, observations, experiments, transactions, etc)  is, given a particular value of $\theta$: 'it provides the chances of each value of $\theta$ having led to that observed value of $x$' [@gamerman_markov_2006, 43]. $\theta$ is a parameter of some kind that describes the shape of a probability distribution (such as mean and variance for normal distributions, $\alpha$, $\beta$ for beta distributions, etc.). The typography of this expression is important. The value $\mathbf{x}$ is bold font because it stands for a random variable, a variable that is observed to take a  range of values (continuous or discrete). The large symbol $\prod_i$ stands for the product or multiplication of the probabilities of all the different observed values of $x$ (not bold),  *given* a particular value of $\theta$. All of this highlights a reversal that stands centre-stage in the contemporary usages of MCMC: the data is evaluated in the light of prior *belief* about the values of the key parameters of the probability distributions. Folding together data and belief through MCMC does not exactly blur what Hacking terms the 'fundamental distinction' [@hacking_taming_1990, 98] between the two faces of probability, the aleatory and the epistemic,  but it certainly entwines them differently.  The epistemic pours out into the aleatory, and the aleatory flows back into the epistemic.  This increased convolution  of the two faces of probability in MCMC comes about only via the many detours of the Markov-chained random variables creeping across the multi-dimensional topography of the joint probability distributions. 

When in 1990, ‘Sampling-Based Approaches to Calculating Marginal Densities’ the article that set off the 'Bayesian revolution' [@robert_introducing_2010, 9], appeared in _Journal of the American Statistical Association_ [@gelfand_sampling-based_1990], the  statisticians Alan Gelfand and Adrian Smith refer to this altered topology. They state that the problem they are addressing is how ‘to obtain numerical estimates of nonanalytically available marginal densities of some or all [the collection of random variables] simply by means of simulated samples from available conditional distributions, and without recourse to sophisticated numerical analytic methods’ [@gelfand_sampling-based_1990, 398]. Their formulation emphasises the mixture of using some things that are accessible to explore things that are not directly accessible. 

They take up the Gibbs sampler algorithm as developed by [@geman_stochastic_1984] for image-processing, investigate some of its formal properties (convergence), and then set out a number of mainstream statistical problems that could be done differently using MCMC and the Gibbs sampler in particular.They show how  MCMC facilitates Bayesian statistical inference  through six illustrative mainstream examples: multinomial models, hierarchical models, multivariate normal sampling, variance components, and the k-group normal means model. The details of these examples need not detain, but each of the illustrations in the paper shows how previously difficult problems of Bayesian inference can be carried out by sampling simulations. As they state in another paper from the same year, ‘the potential of the methodology is enormous, rendering straightforward the analysis of a number of problems hitherto regarded as intractable’ [@gelfand_illustration_1990, 984]. A rapid convergence on MCMC follows from the 1990s onwards. Gibbs samplers appear in desktop computer software such as the widely used WinBUGS ('Windows Bayes Using Gibbs Sampler') written by statisticians at Cambridge University in the early 1990s [@lunn_winbugs-bayesian_2000], and MCMC quickly moves into the different disciplines and applications found today.


## Conclusion

Although widely used, the algorithms, statistical techniques and practices I have been describing are not easily visible or known. Against the common tendency to see MCMC as the reassertion of a neglected interpretation of probability (the degrees of belief version) in the face of its hegemonic treatment in terms of frequencies, I have suggested describing them from a Baroque perspective allows us to hold different interpretations at the same time, without contradiction. Rather than seeing the aleatory and epistemic, world and subject antagonistically in probability, we see  their convoluted embrace in techniques such as MCMC. This embrace can be  seen as a hyper-subjective exteriority, or a hyper-objective interiority depending on one's starting point on the twisted loop running between event and belief. For our purposes, however, neither the objectivist (frequentists) or subjective (Bayesian) interpretations of probability work well. The Bayesian interpretation has the virtue of treating all variables, parameters and numbers as probability distributions. All parameters and parameters in a Bayesian framework can become random variables subject to degrees of belief or credibility.  As a distributive treatment of number, in Bayesian inference, ‘the truth of a variation appears to a subject’ [@deleuze_fold_1993, 20]. While Bayesian statistics seem to me to lie closer to a Baroque treatment of data, understanding probability as ‘subjective’ tends to gloss over the ways in which MCMC supports inference by generating  vast quantities of numbers in the world (the 66,000 nightly runs of Obama's election model). Bayesian statistics still needs numbers from the world. It draws on constant supplies of numbers that vary without an obvious pattern or predictability. This is not a problem with the technique itself, a technique whose capacity to generate numbers in the world has been fabulously productive. But it does suggest that we need to think differently about how numbers happen today. 

My treatment of probability has been more oriented by a Baroque sensibility attuned to the curves, folds, convolutions, mixtures and textures that appear when probabilities multiply. Many of Gilles Deleuze’s formulations of the Baroque converge on curves. He suggests, for instance, that the world ‘is the infinite curve that touches at an infinity of points an infinity of curves, the curve with a unique variable, the convergent series of all series’ [@deleuze_fold_1993, 24]. This description of the world as curve, or the curve of the world, resonates strongly in the scene I have been describing, both in the general account of random variables and multiplying probabilities and in MCMC as a technique that crafts distributive numbers by playing on the convergence of series of numbers. In Deleuze’s account of the fold, curves act as causes: 'the presence of a curved element acts as a cause' [@deleuze_fold_1993, 17]. This claim begins to make more sense as we see curves proliferate, and as we see how finding, generating, exploring and shaping curves become more common practices in so many settings (asthma studies, multiplayer game coordination, epidemiological modelling, spam filtering,etc.). The particles, maps, images and populations comprising our world figure in a Baroque sensibility as curves. When Deleuze writes, following Leibniz, ‘the world is the infinite curve that touches at an infinity of points,’ he could be describing how curves generated by distributive numbers populate our worlds with shapes, textures, images, sounds and movements derived from convergent series of numbers.

Where are we in the folded volumes of data that result? In tracing some of the operative functions that generate curved surfaces in the high-dimensional spaces of contemporary data, we have some chance of finding what in our sensation of change, movement, texture or image is attributable to distributive numbers. MCMC creates convergences between numbers coming from the world, numbers coming from belief or subjects, and numbers that lie somewhere between the world and knowing subject. It would be possible to trace other data-related techniques along the same lines. For instance, the rise of bootstrap sampling as a way of estimating statistical errors [@efron_bootstrap_1979], or the emergence of the Expectation Maximisation (EM) algorithm [@dempster_maximum_1977] would be other perhaps less rich examples of how simulation supports distributive numbers. The example of MCMC offers something that lies closer to the power of curves and the monad as individual number. 

Deleuze describes what how monads include the world in many different ways in the fold. In nearly all of them, the monad is differentiating and integrating: ‘each monad includes the world as infinite series of infinitely small units, but establishes differential relations and integrations only upon a limited portion of the series, such that the monads themselves enter in an infinite series of inverse numbers’ [@deleuze_fold_1993, 130]. The curves that MCMC integrates include a limited portion of the world. Curves express the world (and indeed the state of the world is often expressed as a series of curves: commodity prices, population growth/decline, economic growth, etc.), and curves are in the world: ‘the world is what the soul expresses,’ as Deleuze points out,  just 'because those curves are in the world' [@deleuze_fold_1993, 26]. Underlying much of what I have been describing about distributive numbers, random variables, probability distributions, and the MCMC techniques of sampling from joint probability distributions, there has been some excitation, some tentative desire, to say that it would be good, from the perspective of the Baroque sensibility, not to diminish or flatten these curves, but to find how we are included in them. By this I don’t mean everyone should be using MCMC in order to update their beliefs on various matters. We are all already affected by MCMC in various ways (as for instance, when one of the several hundred million XBox-Live players finds themselves pitted against a new opponent online, the match has been arranged by an MCMC-based player-matching system that is constantly updating its profile of player abilities using game data; or a voter in the US receives a telephone call from a Democrat campaign volunteer; etc).  To the extent that we are monadic, to the extent that we become are 'the most simple numbers',  individual and distributive inverse numbers,

> $\frac{1}{\infty}$ 

that can only be integrated in simulated surfaces and volumes, then the problem becomes how to integrate and differentiate well. What in those operative functions that grid our world can be reused politically and philosophically? 



## References


 

```{r bivar_norm, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Folded surfaces'} 

	source('mcmc_examples.R')
	generate_bivar_norm()
	generate_folded_surface()
```
